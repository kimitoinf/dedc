{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960e01f8300a44fa863e7524a15667f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimitoinf\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import wandb\n",
    "\n",
    "notebook_login()\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed_value = 42):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = './disease_data'\n",
    "for label in os.listdir(dataset_path):\n",
    "    print(label + ': ' + str(len(os.listdir(os.path.join(dataset_path, label)))))\n",
    "dataset = load_dataset(dataset_path, split = 'train').shuffle(seed = 42).train_test_split(test_size = 0.3)\n",
    "split = dataset['test'].train_test_split(test_size = 0.5)\n",
    "dataset['validation'] = split['train']\n",
    "dataset['test'] = split['test']\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset['train'].features['label'].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for loop, label in enumerate(labels):\n",
    "    label2id[label] = str(loop)\n",
    "    id2label[str(loop)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, DefaultDataCollator\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor, RandomVerticalFlip, RandomHorizontalFlip\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "normalize = Normalize(mean = image_processor.image_mean, std = image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), RandomVerticalFlip(), RandomHorizontalFlip(), ToTensor(), normalize])\n",
    "\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.with_transform(transforms)\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis = -1)\n",
    "    accuracy = evaluate.load('accuracy').compute(predictions = predictions, references = labels)\n",
    "    f1_precision_recall = evaluate.combine(['f1', 'precision', 'recall']).compute(predictions = predictions, references = labels, average = 'weighted')\n",
    "    metrics = dict()\n",
    "    for loop in [accuracy, f1_precision_recall]:\n",
    "        metrics.update(loop)\n",
    "    probs = np.array(torch.nn.functional.softmax(torch.tensor(logits), dim = -1).tolist())\n",
    "    classes = list(id2label.values())\n",
    "    true_labels = np.array(labels)\n",
    "    wandb.log({\n",
    "        'roc': wandb.plot.roc_curve(y_true = true_labels, y_probas = probs, labels = classes),\n",
    "        'pr': wandb.plot.pr_curve(y_true = true_labels, y_probas = probs, labels = classes)\n",
    "    })\n",
    "    # wandb.sklearn.plot_confusion_matrix(y_true = true_labels, y_pred = np.array(predictions), labels = classes) # graph is overlapped.\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "from PIL import ImageFile\n",
    "import os\n",
    "import torch\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "os.environ['WANDB_PROJECT'] = 'dedc'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels = len(labels),\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    ")\n",
    "\n",
    "def train():\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = \"./dedc\",\n",
    "        remove_unused_columns = False,\n",
    "        eval_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        learning_rate = 5e-5,\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        per_device_eval_batch_size = 16,\n",
    "        num_train_epochs = 25,\n",
    "        warmup_ratio = 0.1,\n",
    "        logging_steps = 10,\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"accuracy\",\n",
    "        push_to_hub = True,\n",
    "        run_name = 'run',\n",
    "        report_to = 'wandb'\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        data_collator = data_collator,\n",
    "        train_dataset = dataset['train'],\n",
    "        eval_dataset = dataset['validation'],\n",
    "        tokenizer = image_processor,\n",
    "        compute_metrics = compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "\n",
    "train()\n",
    "model_path = './model'\n",
    "image_processor.save_pretrained(model_path)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from tqdm import notebook\n",
    "import wandb\n",
    "\n",
    "wandb.init(project = 'dedc', name = 'test')\n",
    "\n",
    "model_path = './model'\n",
    "classifier = pipeline(\"image-classification\", model = model_path)\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "for loop in notebook.tqdm(dataset['test']):\n",
    "    prediction = classifier(to_pil_image(loop['pixel_values']))\n",
    "    true_labels.append(loop['label'])\n",
    "    pred_labels.append(int(label2id[prediction[0]['label']]))\n",
    "\n",
    "accuracy = evaluate.load('accuracy').compute(predictions = pred_labels, references = true_labels)\n",
    "f1_precision_recall = evaluate.combine(['f1', 'precision', 'recall']).compute(predictions = pred_labels, references = true_labels, average = 'weighted')\n",
    "metrics = dict()\n",
    "for loop in [accuracy, f1_precision_recall]:\n",
    "    metrics.update(loop)\n",
    "wandb.log(metrics)\n",
    "wandb.sklearn.plot_confusion_matrix(y_true = np.array(true_labels), y_pred = np.array(pred_labels), labels = list(id2label.values()))\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kimit-kernel",
   "language": "python",
   "name": "kimit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
